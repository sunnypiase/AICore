{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install swig\n",
    "# !pip install stable-baselines3[extra]\n",
    "# !pip install gymnasium\n",
    "# !pip install stable_baselines3\n",
    "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "# !pip install gymnasium[box2d]\n",
    "# !pip install sb3-contrib\n",
    "# !pip install tensorboard \n",
    "#!pip install scikit-learn \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "import tensorboard\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3 import A2C\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from TraderEnv import TraderEnv\n",
    "from DataProvider import DataProvider\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join('Training', 'Logs')\n",
    "PPO_Path = os.path.join('Training', 'SavedModels', 'PPO_Model_Cartpole')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_provider = DataProvider('Data/Binance_BTCUSDT_2023_minute.csv')\n",
    "df_raw = data_provider.get_raw_data()[110_000:400_000]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_raw['close'], label='Close Price')\n",
    "plt.title('Close Price Chart')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_env = TraderEnv(df_raw, trade_size_dollars=9_000, initial_capital=10_000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the environment\n",
    "env = DummyVecEnv([lambda: trade_env])\n",
    "\n",
    "# Initialize the model\n",
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path)\n",
    "# model = PPO.load(PPO_Path+\"5002\", env=env)\n",
    "\n",
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=10_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(PPO_Path+\"5003\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trade_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = TraderEnv(df_raw[:50000], trade_size_dollars=9_000, initial_capital=10_000)\n",
    "test_env = DummyVecEnv([lambda: test_env])\n",
    "\n",
    "episodes = 5\n",
    "for episode in range(1, episodes + 1):\n",
    "    obs = test_env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        # test_env.render(mode=\"human\")\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, info = test_env.step(action)\n",
    "        score += reward\n",
    "\n",
    "    env_info = info[0]\n",
    "    current_capital = env_info.get('current_capital', 'N/A')\n",
    "    sharpe_ratio = env_info.get('sharpe_ratio', 'N/A')\n",
    "    current_step = env_info.get('current_step', 'N/A')\n",
    "    trades_amount = env_info.get('trades_amount', 'N/A')\n",
    "    print(f'Episode: {episode} Score: {score} Current Capital: {current_capital} Sharpe Ratio: {sharpe_ratio} Current step: {current_step} Trades amount: {trades_amount}')\n",
    "\n",
    "test_env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "# observation, info = env.reset(seed=42)\n",
    "# for _ in range(1000):\n",
    "#    action = env.action_space.sample()  # this is where you would insert your policy\n",
    "#    observation, reward, terminated, truncated, info = env.step(action)\n",
    "#    env.render()\n",
    "\n",
    "#    if terminated or truncated:\n",
    "#       observation, info = env.reset()\n",
    "\n",
    "# env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enviornment_name = \"CartPole-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make(enviornment_name, render_mode=\"human\") # human  rgb_array\n",
    "# env = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model = RecurrentPPO('MlpLstmPolicy', env, verbose=1, tensorboard_log=log_path)\n",
    "# model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.learn(total_timesteps=30_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO_Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_policy(model, env, n_eval_episodes=1, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# episodes = 5\n",
    "# for episode in range(1, episodes+1):\n",
    "#     obs = env.reset()\n",
    "#     done = False\n",
    "#     score = 0\n",
    "    \n",
    "#     while not done:\n",
    "#         env.render()\n",
    "#         action, _ = model.predict(obs)\n",
    "#         obs, reward, done, info = env.step(action)\n",
    "#         score += float(reward)\n",
    "#     print('Episode:{} Score:{}'.format(episode, score))\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_log_path = os.path.join(log_path, 'PPO_6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tensorboard --logdir={training_log_path}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
